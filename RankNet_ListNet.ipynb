{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1435c4-47b3-4a45-9c00-7f72fcfc35dc",
   "metadata": {},
   "source": [
    "<center><h1>From RankNet to ListNet</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e159f-3e87-4037-8fe1-674bf13d7c7d",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb995a-269d-4c26-9446-09b4add74554",
   "metadata": {},
   "source": [
    "In this notebook, we review the paper 'Learning to Rank: From Pairwise Approach to Listwise Approach' and implement the corresponding RankNet and ListNet. Several methods take object pairs as input for learning to rank between the pairs. The paper postulates that learning to rank should adopt the listwise approach instead of pairwise approach. One of the major contributions of the paper is proposed a new probabilistic method for the approach. Specifically, it introdues two probability models:\n",
    "* Permuation probability\n",
    "* Top one probability\n",
    "  \n",
    "These two probability models have been applied to define a listwise loss function during learning the neural networks. \n",
    "\n",
    "The advantages with taking the pairwise approach are:\n",
    "* existing methodologies on classfication can be directly applied (<span style=\"color:blue\">high rank VS low rank is analogous to dog VS cat, meaning existing classification methods can be directly applied to this pairwise ranking problem</span>)\n",
    "* the training instances of document pairs can be easily obtained in certain scenarios (<span style=\"color:blue\">can be obtained easily for pairwise data</span>)\n",
    "\n",
    "The probelms with taking the pairwise approach are:\n",
    "* the objective of learning is formalized as minimizing errors in classification of document pairs, rather than minimizing errors in ranking of documents\n",
    "* the training process is computationally costly, as the number of document pairs is very large (<span style=\"color:blue\">this is kind of permutation: to compare as many pairs as possible</span>)\n",
    "* the assumption of the document pairs are generated i.i.d. is also too strong (<span style=\"color:blue\">this may lead biased of the data</span>)\n",
    "* the number of generated document pairs varies largely from query to query which will lead in training a model biased toward queries with more document pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad2a3d-bd57-4c39-95a9-eed81013e6d4",
   "metadata": {},
   "source": [
    "### Listwise Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78530db0-375d-4d3d-aaca-bff1cfe7a223",
   "metadata": {},
   "source": [
    "This paper proposed a probabilistic method to calculate the listwise loss function. They transform both the scores of the documents assigned by a ranking function and the explicit or implicit judgments of the documents given by humans into probability distributions. Specifically, we consider these two 'score' distribution separately.\n",
    "\n",
    "<b>Firstly</b>, for the socre of the documents, we can have it by humans:\n",
    "\n",
    "Suppose in the training dataset, we have a set of queries $Q = \\{q^{(1)},q^{(2)},...,q^{(m)}\\}$ is given. Each query $q^{i}$ is asscoiated with a list of documents (<span style=\"color:blue\">Similar to hotel booking, a list of hotels is presented for each search</span>) $d^{(i)} = \\Big(d_{1}^{(i)},d_{2}^{(i)},...,d_{n^{(i)}}^{(i)}\\Big)$, where $d_{j}^{(i)}$ denotes the $j-$th document and $n^{(i)}$ denotes the sizes of $d^{(i)}$. Furthermore, each list of documents $d^{(i)}$ is attached with a list of scores $y^{(i)} = \\Big({y_{1}^{(i)},y_{2}^{(i)},...,y_{n^{(i)}}^{(i)}}\\Big)$ where $y_{j}^{(i)}$ denotes the score on document $d_{j}^{(i)}$ with respect to query $q^{(i)}$. And the score $y^{(i)}$ can be explicitly or implicitly given by humans.(<span style=\"color:blue\"> which will be considered as the output during the learning process</span>). For example, $y_{j}^{(i)}$ can be considered as the number of clicks on document $d^{(i)}_j$, which means higher click-on rate is obsered for $d_j^{(i)}$ on $q^{i}$ the higher score (the stronger relevance exists between them) (<span style=\"color:blue\">it can also be given by humans for example review scores</span>).\n",
    "\n",
    "<b>Secondly</b> we consider the score (ranking) assgned by a ranking function (<span style=\"color:blue\">I understand this as the attribute of the document which we need to learn from the neural network</span>), we use a neural network (function) to learn from the input feature vectors. \n",
    "\n",
    "A feature vector $x_j^{(i)} = \\Psi(q^{(i)},d^{(i)}_j)$ is created from each query-document pair $(q^{(i)},d^{(i)}_j)$ for $i=1,2,...,m; j = 1,2,...,n^{(i)}$. Each list of features $x^{(i)} = (x_1^{(i)},x_2^{(i)},...,x_{n^{(i)}}^{(i)})$ where $x_j^{(i)}$ could be (should be) multiple dimension vectors. Then we kind have the input features $x_{(i)}$ and the corresponding output score $y^{(i)}$.\n",
    "\n",
    "<b>Finally</b>, we create a ranking function $f$ (neural network), for each feature vector $x^{(i)}_j$, it provides a score $f(x_{j}^{(i)})$. The scores learned by the neural network $z^{i} = \\Big(f(x_{1}^{(i)}),f(x_{2}^{(i)}),...,f(x_{n^{(i)}}^{(i)})\\Big)$ should align with the human-provided scores, $y^{(i)}= \\Big(y_{1}^{(i)},y_{2}^{(i)},...,y_{n^{(i)}}^{(i)}\\Big)$ in terms of ordering. That is, the order of the values within each vector should be the same.  Then the objective of learning is to define a total loss function to make sure that the order of the values whithin each vector should be minimized:\n",
    "$$\\sum_{i=1}^{m} L(y^{(i)},z^{(i)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19724759-e2c7-4a30-8cb6-424d28d5aa04",
   "metadata": {},
   "source": [
    "### Probability Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bf3af-186f-4798-b9bd-73367a74ac54",
   "metadata": {},
   "source": [
    "In this section, we introduce probability models to calculate the listwise loss function. The authors map a list of scores to a probability distribution using one of the models described here and then apply a metric between two probability distributions as the loss function. The two models are referred to as permutation probability and top-one probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6bd00-ad44-4f8c-bd69-e6a2778e0823",
   "metadata": {},
   "source": [
    "#### Permutation Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e705c-8554-48f0-888c-98be1d8eca95",
   "metadata": {},
   "source": [
    "Suppose the set of objects to be ranked is identified by the numbers $1,2,...,n$. A permutation $\\pi$ of these objects is defined as a bijection from the set $\\{1,2,...,n\\}$ to itself. The permutation is represented as $\\pi = <\\pi(1),\\pi(2),...,\\pi(n)>$, where $\\pi(j)$ denotes the object in the $j-$th position of the permutation. The set of all possible permutations of $n$ objects is denoted by $\\Omega_n$. Use $s$ to denote the list of scores $s=(s_1,s_2,...,s_n)$, where $s_j$ is the score of the $j-$th object. In the article, the authors propose the following definition, lemma and theorems.\n",
    "\n",
    "<b>Definition 1 </b> Suppose that $\\pi$ is a permutation on the $n$ objects, and $\\phi(\\cdot)$ is an increasing and strictly positive function. Then, the probability of permutation $\\pi$ given the list of scores $s$ is defined as \n",
    "$$P_s(\\pi) = \\prod\\limits_{j=1}^{n}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})}$$\n",
    "where $s_{\\pi(j)}$ is the score of object at position $j$ of permutation $\\pi$.\n",
    "\n",
    "<b>Lemma 2 </b> The permutation probabilities $P_s(\\pi)$, $\\pi\\in \\Omega_n$ form a probability distribution over the set of permutations i.e., for each $\\pi \\in \\Omega_n$, we have $P_s(\\pi)>0$, and $\\sum\\limits_{\\pi \\in \\Omega_n} P_s(\\pi) = 1 $.\n",
    "\n",
    "<b>Proof</b>  <span style=\"color:blue\">The proof can be found in the paper as following: </span>\n",
    "\n",
    "According to the definition of $\\phi(\\cdot)$, we have $P_s(\\pi)>0$ for any $\\pi \\in \\Omega_n$. Furthermore,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\pi\\in\\Omega_n} P_s(\\pi) &= \\sum_{\\pi\\in \\Omega_n} \\prod\\limits_{j=1}^{n}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})}\\\\\n",
    "& = \\sum_{\\pi(1)=1}^n    \\sum_{\\pi(2)=1,\\pi(2)\\neq\\pi(1)}^n ...  \\sum_{\\pi(q)=1,\\pi(q)\\neq\\pi(l),\\forall l< q}^n ...  \\sum_{\\pi(n)=1,\\pi(n)\\neq\\pi(l),\\forall l< n}^n  \\prod\\limits_{j=1}^{n}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})}\\tag{1}\\\\\n",
    "& = \\sum_{\\pi(1)=1}^n \\frac{\\phi(s_{\\pi(1)})}{\\sum_{k=1}^{n}\\phi(s_{\\pi(k)})} \\sum_{\\pi(2)=1,\\pi(2)\\neq \\pi(1)}^n \\frac{\\phi(s_{\\pi(2)})}{\\sum_{k=2}^{n}\\phi(s_{\\pi(k)})} ... \\sum_{\\pi(q)=1,\\pi(q)\\neq \\pi(l), \\forall l<q}^n \\frac{\\phi(s_{\\pi(q)})}{\\sum_{k=s}^{n}\\phi(s_{\\pi(k)})} ... \\sum_{\\pi(n)=1,\\pi(n)\\neq \\pi(l), \\forall l<n}^n \\frac{\\phi(s_{\\pi(n)})}{\\sum_{k=n}^{n}\\phi(s_{\\pi(k)})}\n",
    "\\end{align}\n",
    "$$\n",
    "Since for any $1\\leq q\\leq n$, \n",
    "$$\\sum_{\\pi(n)=1,\\pi(n)\\neq \\pi(l), \\forall l<n}^n \\frac{\\phi(s_{\\pi(n)})}{\\sum_{k=n}^{n}\\phi(s_{\\pi(k)})} = 1.$$\n",
    "Then,we have \n",
    "$P_s(\\pi)>0$, and $\\sum\\limits_{\\pi \\in \\Omega_n} P_s(\\pi) = 1 $.\n",
    "\n",
    "<span style=\"color:blue\"> I will prove this lemma by inducation method. </span>\n",
    "\n",
    "<span style=\"color:blue\"> <b>Proof</b>  <span style=\"color:blue\"> I will prove this lemma by inducation method. Firstly: suppose the object size $o=2$ as $\\{1,2\\}$ and the correspong score is $\\{s_1,s_2\\}$. The full permuation is $\\{1,2\\}$ and $\\{2,1\\}$. By Definition 1, we have \n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\pi\\in\\Omega_2} P_s(\\pi) &= \\frac{\\phi(s_1)}{\\phi(s_1)+\\phi(s_2)}\\cdot\\frac{\\phi(s_2)}{\\phi(s_2)} + \\frac{\\phi(s_2)}{\\phi(s_1)+\\phi(s_2)}\\cdot\\frac{\\phi(s_1)}{\\phi(s_1)}\\\\\n",
    "&=\\frac{\\phi(s_1)+\\phi(s_2)}{\\phi(s_1)+\\phi(s_2)}\\\\\n",
    "&= 1\n",
    "\\end{align}\n",
    "$$\n",
    "Now assume when $o=k$, we have $\\sum\\limits_{\\pi \\in \\Omega_k} P_s(\\pi) = 1 $. Then we will prove $\\sum\\limits_{\\pi \\in \\Omega_{k+1}} P_s(\\pi) = 1 $\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\pi\\in\\Omega_{k+1}} P_s(\\pi) &= \\sum\\limits_{\\pi(1)=1, \\pi \\in \\Omega_k} P_s(\\pi)  + \\sum\\limits_{\\pi(1)=2, \\pi \\in \\Omega_k} P_s(\\pi)  + ... + \\sum\\limits_{\\pi(1)=k+1, \\pi \\in \\Omega_k} P_s(\\pi) \\\\\n",
    "&= \\frac{\\phi(s_1)}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})}\\cdot \\sum_{\\pi\\in\\Omega_{k}} P_s(\\pi) + \\frac{\\phi(s_2)}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})}\\cdot \\sum_{\\pi\\in\\Omega_{k}} P_s(\\pi)+ ... + \\frac{\\phi(s_{k+1})}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})}\\cdot \\sum_{\\pi\\in\\Omega_{k}} P_s(\\pi)\n",
    "\\end{align}\n",
    "$$\n",
    "With the assumption $\\sum\\limits_{\\pi \\in \\Omega_k} P_s(\\pi) = 1 $, we can have\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{\\pi\\in\\Omega_{k+1}} P_s(\\pi) &= \\frac{\\phi(s_1)}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})} + \\frac{\\phi(s_2)}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})}+ ...+\\frac{\\phi(s_{k+1})}{\\phi(s_1)+\\phi(s_2)+...+\\phi(s_{k+1})}\\\\\n",
    "&= 1\n",
    "\\end{align}\n",
    "$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd571e6-d8aa-4d29-9e86-241942d93432",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> The contruction of the permutation probability confirms its sum equals to 1. Once the permuation probability is built, we can use it as the probability distribution in the metric function (loss function). The purpose of using this permutation probability can repeatly tell the neural network the order of the score for the document with respect to the query. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda2eea-86e7-4f84-8c0b-152398a47a8e",
   "metadata": {},
   "source": [
    "<b>Theorem 3 </b> Given any two permutations $\\pi$ and $\\pi' \\in \\Omega_n$, if (1) $\\pi(p) = \\pi'(q), p<q$; (2) $\\pi(r) = \\pi'(r)$, $r\\neq p,q$; (3) $s_{\\pi(p)}>s_{\\pi(q)}$, then $P_s(\\pi)>P_s(\\pi')$.\n",
    "\n",
    "<b>Proof</b> From Definition 1, we have \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_s(\\pi) &= \\prod\\limits_{j=1}^{n}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})}\n",
    "\\end{align}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{align}\n",
    "P_s(\\pi') &= \\prod\\limits_{j=1}^{n}\\frac{\\phi(s_{\\pi'(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})}\n",
    "\\end{align}\n",
    "$$\n",
    "In order to prove $P_s(\\pi)>P_s(\\pi')$, we need to prove\n",
    "$$\\prod\\limits_{j=p}^{q}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})} > \\prod\\limits_{j=p}^{q}\\frac{\\phi(s_{\\pi'(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})}$$\n",
    "\n",
    "<span style=\"color:blue\"> With the $\\pi$ permuation   as $(s_{\\pi(1)},s_{\\pi(2)},...,s_{\\pi(p)},...,s_{\\pi(q)},...,s_{\\pi(n)})$; and the $\\pi'$ permuation  as $(s_{\\pi'(1)},s_{\\pi'(1)},...,s_{\\pi'(q)},...,s_{\\pi'(p)},...,s_{\\pi'(n)})$ and $\\pi(r) = \\pi'(r)$, $r\\neq p,q$ we rewrite $P_s(\\pi)$  and $P_s(\\pi')$ as \n",
    "$$\n",
    "\\begin{align}\n",
    "P_s(\\pi) &=  \\frac{\\phi(s_{\\pi(1)})}{\\sum_{k=1}^{n}\\phi(s_{\\pi(k)})}\\cdot \\frac{\\phi(s_{\\pi(2)})}{\\sum_{k=2}^{n}\\phi(s_{\\pi(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi(p)})}{\\sum_{k=p}^{n}\\phi(s_{\\pi(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi(q)})}{\\sum_{k=q}^{n}\\phi(s_{\\pi(k)})}\\cdot ... \\cdot \\frac{\\phi(s_{\\pi(n)})}{\\phi(s_{\\pi(n)})}\n",
    "\\end{align}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{align}\n",
    "P_s(\\pi') &=  \\frac{\\phi(s_{\\pi'(1)})}{\\sum_{k=1}^{n}\\phi(s_{\\pi'(k)})}\\cdot \\frac{\\phi(s_{\\pi'(2)})}{\\sum_{k=2}^{n}\\phi(s_{\\pi'(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi'(q)})}{\\sum_{k=p}^{n}\\phi(s_{\\pi'(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi'(p)})}{\\sum_{k=q}^{n}\\phi(s_{\\pi'(k)})}\\cdot ... \\cdot \\frac{\\phi(s_{\\pi'(n)})}{\\phi(s_{\\pi'(n)})}\\\\\n",
    "&=  \\frac{\\phi(s_{\\pi(1)})}{\\sum_{k=1}^{n}\\phi(s_{\\pi(k)})}\\cdot \\frac{\\phi(s_{\\pi(2)})}{\\sum_{k=2}^{n}\\phi(s_{\\pi(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi'(q)})}{\\sum_{k=p}^{n}\\phi(s_{\\pi'(k)})}\\cdot ... \\cdot\\frac{\\phi(s_{\\pi'(p)})}{\\sum_{k=q}^{n}\\phi(s_{\\pi'(k)})}\\cdot ... \\cdot \\frac{\\phi(s_{\\pi(n)})}{\\phi(s_{\\pi(n)})}\n",
    "\\end{align}\n",
    "$$\n",
    "Now it is clear why only need to prove \n",
    "$$\\prod\\limits_{j=p}^{q}\\frac{\\phi(s_{\\pi(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})} > \\prod\\limits_{j=p}^{q}\\frac{\\phi(s_{\\pi'(j)})}{\\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})}$$\n",
    "</span>. \n",
    "\n",
    "Notice that $$\\prod\\limits_{j=p}^{q} \\phi(s_{\\pi(j)})  = \\prod\\limits_{j=p}^{q} \\phi(s_{\\pi'(j)}) $$. Thus, we need to prove \n",
    "$$\\prod\\limits_{j=p}^{q}\\frac{1}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})} > \\prod\\limits_{j=p}^{q}\\frac{1}{\\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})}$$\n",
    "When $j = p$, $\\sum_{k=j}^{n}\\phi(s_{\\pi(k)}) =  \\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})$. \n",
    "When $p<j <q$, because $\\phi$ is an increasing function and $s_{\\pi(p)}>s_{\\pi(q)}$, we have \n",
    "$$\\prod\\limits_{j=p+1}^{q}\\frac{1}{\\sum_{k=j}^{n}\\phi(s_{\\pi(k)})} > \\prod\\limits_{j=p+1}^{q}\\frac{1}{\\sum_{k=j}^{n}\\phi(s_{\\pi'(k)})}$$ \n",
    "Then we have $P_s(\\pi)>P_s(\\pi')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b17d8-e35d-4e4f-a73a-fcdce029e635",
   "metadata": {},
   "source": [
    "<b>Theorem 4 </b> For the $n$ objects, if $s_1>s_2>...>s_n$, then $P_s(<1,2,...,n>)$ is the highest permutation probability and $P_s(<n,n-1,...,1>)$ is the lowest permutation probability among the permutation probabilities of the $n$ objects.\n",
    "\n",
    "<span style=\"color:blue\"> This could be proved with Theorem 3. </span>\n",
    "\n",
    "<span style=\"color:blue\"> All the lemma and theorems are used to confirm that the permuation probability satisfies the probability distribution and better ranking will give higher probability. But this may lead time consuming to compute the permutation of the object.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e08d79-2286-4bb3-a769-1821eb3262c3",
   "metadata": {},
   "source": [
    "#### Top One Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af81655-376d-43f0-8719-93f7fc47f080",
   "metadata": {},
   "source": [
    "The top one probability of an object represents the probability of its being ranked on the top, given the scores of all the objects.\n",
    "\n",
    "<b>Definition 5 </b> The top one probability of object $j$ is defined as  \n",
    "$$P_s(j) = \\sum_{\\pi(1) = j, \\pi \\in \\Omega_n} P_s(\\pi)$$\n",
    "where $P_s(\\pi)$ is  permutation probability of $\\pi$ given $s$.\n",
    "\n",
    "<span style=\"color:blue\"> Here it shows that we still need to cacluate all the permutation probabilities. In order to solve this problem, the authors provide an alternative way to define the probability.</span>\n",
    "\n",
    "\n",
    "<b>Theorem 6 </b> For top one probability $P_s(j)$, we have \n",
    "$$P_s(j) = \\frac{\\phi(s_j)}{\\sum_{k=1}^{n}\\phi(s_k)},$$\n",
    "where $s_j$ is the score of object $j$, $j=1,2,...,n.$\n",
    "\n",
    "<b>Proof </b> <span style=\"color:blue\"> We first write $P_s(j)$ by the definition of $P_s(\\pi)$. \n",
    "$$\n",
    "\\begin{align}\n",
    "P_s(j) &= \\sum_{\\pi(1) = j, \\pi \\in \\Omega_n} P_s(\\pi)\\\\\n",
    "&= \\frac{\\phi(s_{j})}{\\sum_{k=1}^{n}\\phi(s_{k})}  \\sum_{\\pi \\in \\Omega_{n-1}} P_s(\\pi)\\\\\n",
    "&= \\frac{\\phi(s_{j})}{\\sum_{k=1}^{n}\\phi(s_{k})}\n",
    "\\end{align}\n",
    "$$\n",
    "The last can be obtained by Lemma 2.\n",
    "</span> \n",
    "\n",
    "With Theorem 6, we can consequently have the following lemma and theorem.\n",
    "\n",
    "<b>Lemma 7 </b> Top one probabilities $P_s(j)$, $j=1,2,...,n$ forms a probability distribution over the set of $n$ objects.\n",
    "\n",
    "<b>Theorem 8 </b> Given any two objects $j$ and $k$, if $s_j>s_k$, $j\\neq k$, $j,k=1,2,...,n$ then  $P_s(j)>P_s(k)$.\n",
    "\n",
    "\n",
    "If we use Cross Entropy as metric, the listwise loss function becomes:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(y^{(i)},z^{(i)}) = -\\sum_{j=1}^{n} P_{y^{(i)}}(j)\\log(P_{z^{(i)}}(j))\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e24831-8318-4c1e-a38a-1fd76c2d5510",
   "metadata": {},
   "source": [
    "### RankNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77efc9-7513-43b9-b20a-9f3114560934",
   "metadata": {},
   "source": [
    "#### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b876d8-109e-46fb-b739-298b31ff5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f56b32-266a-488f-8a95-df09303a7c7c",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bbec5c-6859-464a-9180-6d50aeb58460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output a single score\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7d73ec-0470-4380-8625-0001883067d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranknet_loss(score1, score2, true_label):\n",
    "  \n",
    "    #The RankNet loss\n",
    "    s_diff = score1 - score2\n",
    "    prob = torch.sigmoid(s_diff)\n",
    "    \n",
    "    loss = nn.BCELoss()(prob, true_label)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7441f40b-a8c4-46dc-b42c-1058106c6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5  # Assume each item has 5 features\n",
    "model = RankNet(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f75835-c9b6-471c-b1ea-5c632b2c4567",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6b5d2a-507d-444a-b635-ea36901fc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "item1_features = torch.randn((10, input_size))  # 10 items with 5 features\n",
    "item2_features = torch.randn((10, input_size))\n",
    "true_labels = torch.randint(0, 2, (10, 1)).float()\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2354-58e9-4bbb-8667-04f6572bb429",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916e3696-5aba-4f8d-acbd-6b9bc6eb783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6771\n",
      "Epoch 10, Loss: 0.6262\n",
      "Epoch 20, Loss: 0.5824\n",
      "Epoch 30, Loss: 0.5428\n",
      "Epoch 40, Loss: 0.5052\n",
      "Epoch 50, Loss: 0.4685\n",
      "Epoch 60, Loss: 0.4326\n",
      "Epoch 70, Loss: 0.3977\n",
      "Epoch 80, Loss: 0.3639\n",
      "Epoch 90, Loss: 0.3309\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    \n",
    "    # Get scores for both items\n",
    "    score1 = model(item1_features)\n",
    "    score2 = model(item2_features)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = ranknet_loss(score1, score2, true_labels)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473e33b-a56f-49c3-b5a2-4538c63c00b5",
   "metadata": {},
   "source": [
    "### ListNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e3dfd3-9225-4562-ae3f-678d47a59591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4be0917-e0a9-498b-a21c-fc2cd452c6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "n_samples, n_features = 100, 10\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "relevance_scores = np.random.randint(0, 5, size=(n_samples,))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, relevance_scores, test_size=0.2, random_state=42)\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5136d80b-436c-4fd1-bbdb-a3d58968ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ListNet Model\n",
    "class ListNetModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ListNetModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299a0af9-a6af-47f4-8325-b916cedbc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listnet_loss(y_pred, y_true):\n",
    "    P_y_pred = F.softmax(y_pred, dim=0)\n",
    "    P_y_true = F.softmax(y_true, dim=0)\n",
    "    loss = -torch.sum(P_y_true * torch.log(P_y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f6dcb8a-3f72-44cf-a390-f65d22c243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapped_pairs(ys_pred, ys_target):\n",
    "    N = ys_target.shape[0]\n",
    "    swapped = 0\n",
    "    for i in range(N - 1):\n",
    "        for j in range(i + 1, N):\n",
    "            if ys_target[i] < ys_target[j]:\n",
    "                if ys_pred[i] > ys_pred[j]:\n",
    "                    swapped += 1\n",
    "            elif ys_target[i] > ys_target[j]:\n",
    "                if ys_pred[i] < ys_pred[j]:\n",
    "                    swapped += 1\n",
    "    return swapped\n",
    "\n",
    "\n",
    "def ndcg(ys_true, ys_pred):\n",
    "    def dcg(ys_true, ys_pred):\n",
    "        _, argsort = torch.sort(ys_pred, descending=True, dim=0)\n",
    "        ys_true_sorted = ys_true[argsort]\n",
    "        ret = 0\n",
    "        for i, l in enumerate(ys_true_sorted, 1):\n",
    "            ret += (2 ** l - 1) / np.log2(1 + i)\n",
    "        return ret\n",
    "    ideal_dcg = dcg(ys_true, ys_true)\n",
    "    pred_dcg = dcg(ys_true, ys_pred)\n",
    "    return pred_dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa5288b-4260-4fc5-a2cb-76f79b4fdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = ListNetModel(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b37dd27-6269-486d-b2ec-76fa38088f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 3.8510\n",
      "epoch: 50 valid swapped pairs: 61/190 ndcg: 0.7927\n",
      "Epoch [100/200], Loss: 3.8577\n",
      "epoch: 100 valid swapped pairs: 61/190 ndcg: 0.7922\n",
      "Epoch [150/200], Loss: 3.8483\n",
      "epoch: 150 valid swapped pairs: 63/190 ndcg: 0.7732\n",
      "Epoch [200/200], Loss: 3.8553\n",
      "epoch: 200 valid swapped pairs: 64/190 ndcg: 0.7802\n"
     ]
    }
   ],
   "source": [
    "num_epochs=200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predicted_scores = model(X_train).squeeze()\n",
    "    loss = listnet_loss(predicted_scores, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test)\n",
    "            valid_swapped_pairs = swapped_pairs(test_pred, y_test)\n",
    "            ndcg_score = ndcg(y_test, test_pred).item()\n",
    "            print(f\"epoch: {epoch + 1} valid swapped pairs: {valid_swapped_pairs}/{y_test.shape[0] * (y_test.shape[0] - 1) // 2} ndcg: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cbe56-2b46-4a20-972b-fc99f1a3fc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
